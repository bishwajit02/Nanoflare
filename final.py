# === ROBUST MULTI-GAUSSIAN PEAK DECOMPOSITION ===

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import signal
from scipy.optimize import curve_fit
from scipy.ndimage import gaussian_filter1d
import pandas as pd
import argparse
import warnings
warnings.filterwarnings('ignore')

# === CONFIGURATION ===
# You can easily modify these parameters to analyze different datasets
DATA_FILE = 'data/2024_xrsa_xrsb.csv'  # Change this to analyze different years
START_LINE = 0                       # Starting line for data trimming
END_LINE = 360                        # Ending line for data trimming (500 samples)

# Available data files:
# - data/2017_xrsa_xrsb.csv (2017 GOES data)
# - data/2018_xrsa_xrsb.csv (2018 GOES data)
# - data/2019_xrsa_xrsb.csv (2019 GOES data)
# - data/2020_xrsa_xrsb.csv (2020 GOES data)
# - data/2021_xrsa_xrsa.csv (2021 GOES data)
# - data/2022_xrsa_xrsb.csv (2022 GOES data)
# - data/2023_xrsa_xrsb.csv (2023 GOES data)
# - data/2024_xrsa_xrsb.csv (2024 GOES data)
# - data/2025_xrsa_xrsb.csv (2025 GOES data)

# === DATA LOADING AND BASELINE CORRECTION ===

def load_and_preprocess_data(data_file=DATA_FILE, start_line=START_LINE, end_line=END_LINE):
    """
    Load GOES X-ray data and apply baseline correction
    
    Parameters:
    -----------
    data_file : str
        Path to the CSV data file
    start_line : int
        Starting line for data trimming (default: 2500)
    end_line : int
        Ending line for data trimming (default: 3000)
    
    Returns:
    --------
    tuple : (time_minutes, xrsa_corrected, xrsb_corrected)
    """
    print("üìÅ Loading GOES X-ray data...")
    
    try:
        # Load the data
        df = pd.read_csv(data_file)
        print(f"‚úÖ Loaded {len(df)} data points from {data_file}")
        
        # Clean missing values
        df_clean = df.dropna()
        print(f"‚úÖ Cleaned data: {len(df_clean)} points after removing NaN values")
        
        # Trim to specified range
        df_trimmed = df_clean.iloc[start_line:end_line].copy()
        print(f"‚úÖ Trimmed to {len(df_trimmed)} points (lines {start_line}-{end_line})")
        
        # Extract time and flux data
        time_minutes = df_trimmed['time_minutes'].values
        xrsa_flux = df_trimmed['xrsa_flux_observed'].values
        xrsb_flux = df_trimmed['xrsb_flux_observed'].values
        
        # Apply log transformation with small epsilon to avoid log(0)
        epsilon = 1e-10
        log_xrsa = np.log10(np.maximum(xrsa_flux, epsilon))
        log_xrsb = np.log10(np.maximum(xrsb_flux, epsilon))
        
        print("‚úÖ Applied log transformation to flux data")
        
        # Baseline correction using Asymmetric Least Squares
        print("üîÑ Applying baseline correction...")
        
        try:
            from pybaselines import Baseline
            baseline_fitter = Baseline(time_minutes)
            
            # Apply AsLS baseline correction
            xrsa_result = baseline_fitter.asls(log_xrsa, lam=1e7, p=0.01)
            xrsb_result = baseline_fitter.asls(log_xrsb, lam=1e7, p=0.01)
            
            xrsa_baseline = xrsa_result[0]
            xrsb_baseline = xrsb_result[0]
            
            xrsa_corrected = log_xrsa - xrsa_baseline
            xrsb_corrected = log_xrsb - xrsb_baseline
            
            print("‚úÖ Baseline correction completed using pybaselines AsLS method")
            
        except ImportError:
            print("‚ö†Ô∏è  pybaselines not available, using simple detrending...")
            # Fallback to simple linear detrending
            p_xrsa = np.poly1d(np.polyfit(time_minutes, log_xrsa, 1))
            p_xrsb = np.poly1d(np.polyfit(time_minutes, log_xrsb, 1))
            
            xrsa_corrected = log_xrsa - p_xrsa(time_minutes)
            xrsb_corrected = log_xrsb - p_xrsb(time_minutes)
            
            print("‚úÖ Applied linear detrending as fallback")
        
        # Print statistics
        print(f"\nüìä Data Statistics:")
        print(f"   Time range: {time_minutes[0]:.1f} to {time_minutes[-1]:.1f} minutes")
        print(f"   Duration: {(time_minutes[-1] - time_minutes[0])/60:.1f} hours")
        print(f"   XRSA corrected std: {np.std(xrsa_corrected):.6f}")
        print(f"   XRSB corrected std: {np.std(xrsb_corrected):.6f}")
        
        return time_minutes, xrsa_corrected, xrsb_corrected
        
    except Exception as e:
        print(f"‚ùå Error loading data: {e}")
        print("Please ensure the data file exists and is accessible")
        return None, None, None

# === CORRECTED GAUSSIAN FUNCTIONS ===

def gaussian(x, amplitude, center, width):
    """Single Gaussian function with proper array handling"""
    x = np.asarray(x, dtype=np.float64)
    return amplitude * np.exp(-0.5 * ((x - center) / np.abs(width)) ** 2)

def multi_gaussian(x, *params):
    """Multiple Gaussian function with corrected parameter handling"""
    x = np.asarray(x, dtype=np.float64)
    n_gaussians = len(params) // 3
    result = np.zeros_like(x, dtype=np.float64)
    
    for i in range(n_gaussians):
        amplitude = params[i*3]
        center = params[i*3 + 1]
        width = np.abs(params[i*3 + 2])  # Ensure positive width
        if width > 0:  # Avoid division by zero
            result += amplitude * np.exp(-0.5 * ((x - center) / width) ** 2)
    
    return result

# === ENHANCED PEAK DETECTION ===

def find_significant_peaks(data, time, prominence_factor=2.0, min_distance=8):
    """Robust peak detection with better parameters"""
    data = np.asarray(data, dtype=np.float64)
    
    # Calculate adaptive threshold
    data_std = np.std(data[np.isfinite(data)])
    data_mean = np.mean(data[np.isfinite(data)])
    prominence_threshold = max(prominence_factor * data_std, 0.001)
    
    # Find peaks
    peaks, properties = signal.find_peaks(
        data, 
        prominence=prominence_threshold,
        distance=min_distance,
        width=2,
        height=data_mean + prominence_threshold/3
    )
    
    return peaks, properties

def extract_peak_region(data, time, peak_idx, window_size=60):
    """Extract region around peak with bounds checking"""
    data = np.asarray(data, dtype=np.float64)
    start_idx = max(0, peak_idx - window_size//2)
    end_idx = min(len(data), peak_idx + window_size//2)
    
    # Ensure we have enough points
    if end_idx - start_idx < 10:
        center = peak_idx
        start_idx = max(0, center - 15)
        end_idx = min(len(data), center + 15)
    
    return start_idx, end_idx

# === SIMPLIFIED BUT ROBUST FITTING FUNCTIONS ===

def fit_single_gaussian(x, y):
    """Robust single Gaussian fitting"""
    try:
        x = np.asarray(x, dtype=np.float64)
        y = np.asarray(y, dtype=np.float64)
        
        # Remove any infinite or NaN values
        mask = np.isfinite(x) & np.isfinite(y)
        x_clean = x[mask]
        y_clean = y[mask]
        
        if len(x_clean) < 4:
            return None, None, 0, np.zeros_like(y)
        
        # Better initial parameter estimation
        peak_idx = np.argmax(y_clean)
        amplitude_init = y_clean[peak_idx] - np.min(y_clean)
        center_init = x_clean[peak_idx]
        
        # Estimate width from half-maximum points
        half_max = (np.max(y_clean) + np.min(y_clean)) / 2
        indices = np.where(y_clean >= half_max)[0]
        if len(indices) > 1:
            width_estimate = (x_clean[indices[-1]] - x_clean[indices[0]]) / 2.355
        else:
            width_estimate = (x_clean[-1] - x_clean[0]) / 6
        
        width_init = max(width_estimate, (x_clean[-1] - x_clean[0]) / 20)
        
        # Initial parameters
        p0 = [amplitude_init, center_init, width_init]
        
        # Parameter bounds
        bounds = (
            [0, x_clean[0] - (x_clean[-1] - x_clean[0]), 0.001],  # lower bounds
            [amplitude_init * 5, x_clean[-1] + (x_clean[-1] - x_clean[0]), x_clean[-1] - x_clean[0]]  # upper bounds
        )
        
        # Fit the Gaussian
        popt, pcov = curve_fit(gaussian, x_clean, y_clean, p0=p0, bounds=bounds, maxfev=2000)
        
        # Generate fit curve for all original x points
        y_fit = gaussian(x, *popt)
        
        # Calculate R-squared
        y_pred_clean = gaussian(x_clean, *popt)
        ss_res = np.sum((y_clean - y_pred_clean) ** 2)
        ss_tot = np.sum((y_clean - np.mean(y_clean)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        return popt, pcov, r2, y_fit
        
    except Exception as e:
        print(f"Single Gaussian fitting error: {e}")
        return None, None, 0, np.zeros_like(y)

def fit_double_gaussian(x, y):
    """Robust double Gaussian fitting with improved initialization"""
    try:
        x = np.asarray(x, dtype=np.float64)
        y = np.asarray(y, dtype=np.float64)
        
        # Clean data
        mask = np.isfinite(x) & np.isfinite(y)
        x_clean = x[mask]
        y_clean = y[mask]
        
        if len(x_clean) < 8:
            return None, None, 0, np.zeros_like(y)
        
        # Find main peak
        main_peak_idx = np.argmax(y_clean)
        main_amp = y_clean[main_peak_idx] - np.min(y_clean)
        main_center = x_clean[main_peak_idx]
        
        # Estimate main peak width
        half_max = (np.max(y_clean) + np.min(y_clean)) / 2
        indices = np.where(y_clean >= half_max)[0]
        if len(indices) > 1:
            main_width = (x_clean[indices[-1]] - x_clean[indices[0]]) / 4
        else:
            main_width = (x_clean[-1] - x_clean[0]) / 10
        
        main_width = max(main_width, (x_clean[-1] - x_clean[0]) / 20)
        
        # Find secondary peak by smoothing and looking for local maxima
        y_smooth = gaussian_filter1d(y_clean, sigma=.01)
        
        # Look for secondary peaks
        sec_peaks, _ = signal.find_peaks(y_smooth, height=np.max(y_smooth) * 0.2, distance=len(y_smooth)//4)
        
        if len(sec_peaks) > 1:
            # Remove the main peak from candidates
            sec_peaks = sec_peaks[sec_peaks != main_peak_idx]
            if len(sec_peaks) > 0:
                # Choose the most prominent secondary peak
                sec_heights = y_smooth[sec_peaks]
                sec_peak_idx = sec_peaks[np.argmax(sec_heights)]
            else:
                # Create artificial secondary peak
                sec_peak_idx = len(x_clean)//3 if main_peak_idx > len(x_clean)//2 else 2*len(x_clean)//3
        else:
            # Create artificial secondary peak
            sec_peak_idx = len(x_clean)//3 if main_peak_idx > len(x_clean)//2 else 2*len(x_clean)//3
        
        # Secondary peak parameters
        sec_peak_idx = min(sec_peak_idx, len(x_clean)-1)
        sec_amp = main_amp * 0.5
        sec_center = x_clean[sec_peak_idx]
        sec_width = main_width * 1.5
        
        # Initial parameters for double Gaussian
        p0 = [main_amp, main_center, main_width, sec_amp, sec_center, sec_width]
        
        # Parameter bounds
        x_range = x_clean[-1] - x_clean[0]
        bounds = (
            [0, x_clean[0] - x_range, 0.001, 0, x_clean[0] - x_range, 0.001],  # lower bounds
            [main_amp * 3, x_clean[-1] + x_range, x_range, main_amp * 2, x_clean[-1] + x_range, x_range]  # upper bounds
        )
        
        # Fit the double Gaussian
        popt, pcov = curve_fit(multi_gaussian, x_clean, y_clean, p0=p0, bounds=bounds, maxfev=5000)
        
        # Generate fit curve for all original x points
        y_fit = multi_gaussian(x, *popt)
        
        # Calculate R-squared
        y_pred_clean = multi_gaussian(x_clean, *popt)
        ss_res = np.sum((y_clean - y_pred_clean) ** 2)
        ss_tot = np.sum((y_clean - np.mean(y_clean)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        return popt, pcov, r2, y_fit
        
    except Exception as e:
        print(f"Double Gaussian fitting error: {e}")
        return None, None, 0, np.zeros_like(y)

def fit_triple_gaussian(x, y):
    """Robust triple Gaussian fitting"""
    try:
        x = np.asarray(x, dtype=np.float64)
        y = np.asarray(y, dtype=np.float64)
        
        # Clean data
        mask = np.isfinite(x) & np.isfinite(y)
        x_clean = x[mask]
        y_clean = y[mask]
        
        if len(x_clean) < 12:
            return None, None, 0, np.zeros_like(y)
        
        # Main peak parameters
        main_peak_idx = np.argmax(y_clean)
        main_amp = y_clean[main_peak_idx] - np.min(y_clean)
        main_center = x_clean[main_peak_idx]
        base_width = (x_clean[-1] - x_clean[0]) / 12
        
        # Create three components with reasonable spacing
        p0 = [
            main_amp, main_center, base_width,  # Main component
            main_amp * 0.6, x_clean[len(x_clean)//4], base_width * 1.2,  # Left component
            main_amp * 0.4, x_clean[3*len(x_clean)//4], base_width * 1.4   # Right component
        ]
        
        # Parameter bounds
        x_range = x_clean[-1] - x_clean[0]
        bounds = (
            [0, x_clean[0] - x_range, 0.001] * 3,  # lower bounds for all 3 components
            [main_amp * 3, x_clean[-1] + x_range, x_range] * 3  # upper bounds for all 3 components
        )
        
        # Fit the triple Gaussian
        popt, pcov = curve_fit(multi_gaussian, x_clean, y_clean, p0=p0, bounds=bounds, maxfev=8000)
        
        # Generate fit curve
        y_fit = multi_gaussian(x, *popt)
        
        # Calculate R-squared
        y_pred_clean = multi_gaussian(x_clean, *popt)
        ss_res = np.sum((y_clean - y_pred_clean) ** 2)
        ss_tot = np.sum((y_clean - np.mean(y_clean)) ** 2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0
        
        return popt, pcov, r2, y_fit
        
    except Exception as e:
        print(f"Triple Gaussian fitting error: {e}")
        return None, None, 0, np.zeros_like(y)

# === ENHANCED STYLING ===
plt.rcdefaults()

# Professional color palette
COLORS = {
    'data': '#2C3E50',           # Dark blue-gray
    'peak_marker': '#E74C3C',    # Red
    'gaussian1': '#3498DB',      # Blue
    'gaussian2': "#CC2E2E",      # Green  
    'gaussian3': "#FFA20B",      # Orange
    'total_fit': '#8E44AD',      # Purple
    'background': '#FAFAFA',     # Light gray
    'grid': '#ECF0F1',          # Very light gray
    'text': '#34495E'           # Dark gray
}

plt.style.use('default')

# === MAIN ANALYSIS ===

# Parse command line arguments
parser = argparse.ArgumentParser(description='Multi-Gaussian Solar Flare Analysis')
parser.add_argument('--data', type=str, default=DATA_FILE, 
                   help=f'Data file path (default: {DATA_FILE})')
parser.add_argument('--start', type=int, default=START_LINE,
                   help=f'Starting line for data trimming (default: {START_LINE})')
parser.add_argument('--end', type=int, default=END_LINE,
                   help=f'Ending line for data trimming (default: {END_LINE})')
parser.add_argument('--show-config', action='store_true',
                   help='Show current configuration and exit')

args = parser.parse_args()

# Show configuration if requested
if args.show_config:
    print("üîß CURRENT CONFIGURATION:")
    print(f"   Data file: {args.data}")
    print(f"   Start line: {args.start}")
    print(f"   End line: {args.end}")
    print(f"   Sample size: {args.end - args.start}")
    print("\nAvailable data files:")
    import os
    data_dir = 'data'
    if os.path.exists(data_dir):
        for file in os.listdir(data_dir):
            if file.endswith('.csv'):
                print(f"   - {os.path.join(data_dir, file)}")
    exit()

print("üî¨ ROBUST MULTI-GAUSSIAN PEAK DECOMPOSITION")
print("=" * 65)

# Load and preprocess data with command line arguments
time_minutes, xrsa_corrected, xrsb_corrected = load_and_preprocess_data(
    data_file=args.data, start_line=args.start, end_line=args.end
)

# Check if data loading was successful
if time_minutes is None:
    print("‚ùå Failed to load data. Exiting analysis.")
    exit()

print("‚úÖ Data loaded and preprocessed successfully. Starting analysis...")

# Initialize results storage
analysis_results = {}

# === PEAK DETECTION PHASE ===
print(f"\nüìä Peak Detection:")

for channel_name, data in [('xrsa', xrsa_corrected), ('xrsb', xrsb_corrected)]:
    try:
        print(f"\n   {channel_name.upper()} Channel:")
        
        # Ensure data is clean
        data = np.asarray(data, dtype=np.float64)
        peaks, props = find_significant_peaks(data, time_minutes)
        
        print(f"   ‚Ä¢ Found {len(peaks)} significant peaks")
        
        if len(peaks) > 0:
            # Get highest peak
            peak_amplitudes = data[peaks]
            highest_idx = np.argmax(peak_amplitudes)
            highest_peak_pos = peaks[highest_idx]
            
            # Extract peak region
            start_idx, end_idx = extract_peak_region(data, time_minutes, highest_peak_pos, window_size=60)
            
            peak_time = time_minutes[start_idx:end_idx]
            peak_data = data[start_idx:end_idx]
            
            print(f"   ‚Ä¢ Highest peak at t={time_minutes[highest_peak_pos]:.1f} min")
            print(f"   ‚Ä¢ Peak amplitude: {peak_amplitudes[highest_idx]:.4f}")
            print(f"   ‚Ä¢ Analysis region: {len(peak_data)} points")
            
            analysis_results[channel_name] = {
                'all_peaks': peaks,
                'peak_time': peak_time,
                'peak_data': peak_data,
                'peak_position': highest_peak_pos,
                'peak_amplitude': peak_amplitudes[highest_idx],
                'full_data': data,
                'full_time': time_minutes
            }
            
        else:
            print(f"   ‚Ä¢ No significant peaks detected")
            
    except Exception as e:
        print(f"   ‚ùå {channel_name.upper()} analysis failed: {e}")

# === GAUSSIAN FITTING PHASE ===
print(f"\nüî¨ Multi-Gaussian Fitting:")

for channel in analysis_results.keys():
    print(f"\n   {channel.upper()} Channel Fitting:")
    
    peak_time = analysis_results[channel]['peak_time']
    peak_data = analysis_results[channel]['peak_data']
    
    if len(peak_data) < 10:
        print(f"   ‚ö† Insufficient data points ({len(peak_data)} points)")
        continue
    
    # Store fitting results
    fitting_results = {}
    
    # Single Gaussian
    print("   ‚Ä¢ Fitting Single Gaussian...", end=' ')
    single_params, single_cov, single_r2, single_fit = fit_single_gaussian(peak_time, peak_data)
    
    if single_params is not None:
        fitting_results['single'] = {
            'params': single_params,
            'r2': single_r2,
            'fit': single_fit,
            'n_components': 1
        }
        print(f"‚úÖ R¬≤={single_r2:.4f}")
    else:
        print("‚ùå Failed")
    
    # Double Gaussian
    print("   ‚Ä¢ Fitting Double Gaussian...", end=' ')
    double_params, double_cov, double_r2, double_fit = fit_double_gaussian(peak_time, peak_data)
    
    if double_params is not None:
        fitting_results['double'] = {
            'params': double_params,
            'r2': double_r2,
            'fit': double_fit,
            'n_components': 2
        }
        print(f"‚úÖ R¬≤={double_r2:.4f}")
    else:
        print("‚ùå Failed")
    
    # Triple Gaussian
    print("   ‚Ä¢ Fitting Triple Gaussian...", end=' ')
    triple_params, triple_cov, triple_r2, triple_fit = fit_triple_gaussian(peak_time, peak_data)
    
    if triple_params is not None:
        fitting_results['triple'] = {
            'params': triple_params,
            'r2': triple_r2,
            'fit': triple_fit,
            'n_components': 3
        }
        print(f"‚úÖ R¬≤={triple_r2:.4f}")
    else:
        print("‚ùå Failed")
    
    # Determine best model based on R¬≤ improvement
    if fitting_results:
        r2_values = {model: result['r2'] for model, result in fitting_results.items()}
        best_model = max(r2_values.keys(), key=lambda x: r2_values[x])
        
        print(f"   ‚úÖ Best model: {best_model.capitalize()} Gaussian (R¬≤={r2_values[best_model]:.4f})")
        
        analysis_results[channel]['fitting_results'] = fitting_results
        analysis_results[channel]['best_model'] = best_model
    else:
        print(f"   ‚ùå All fits failed")

# === ENHANCED VISUALIZATION ===

# Create comprehensive figure
fig = plt.figure(figsize=(20, 14))
fig.patch.set_facecolor('white')

# Title
fig.suptitle('Multi-Gaussian Solar Flare Analysis\n' +
             'Component Decomposition ‚Ä¢ Peak Structure Analysis ‚Ä¢ GOES X-ray Data', 
             fontsize=18, fontweight='bold', color=COLORS['text'], y=0.95)

# Grid layout
gs = fig.add_gridspec(3, 4, hspace=0.35, wspace=0.25, 
                      left=0.06, right=0.96, top=0.88, bottom=0.08)

# === ROW 1: FULL SIGNAL OVERVIEW ===
for i, channel in enumerate(['xrsa', 'xrsb']):
    if channel in analysis_results:
        ax = fig.add_subplot(gs[0, i*2:(i+1)*2])
        ax.set_facecolor(COLORS['background'])
        
        full_data = analysis_results[channel]['full_data']
        full_time = analysis_results[channel]['full_time']
        
        # Plot full corrected signal
        ax.plot(full_time, full_data, color=COLORS['data'], linewidth=2, alpha=0.8,
                label=f'{channel.upper()} Corrected Signal')
        
        # Mark detected peaks
        peaks = analysis_results[channel]['all_peaks']
        ax.plot(full_time[peaks], full_data[peaks], 'o', 
                color=COLORS['peak_marker'], markersize=8, 
                label=f'Detected Peaks ({len(peaks)})', zorder=5)
        
        # Highlight analysis region
        peak_time = analysis_results[channel]['peak_time']
        ax.axvspan(peak_time[0], peak_time[-1], alpha=0.3, 
                   color=COLORS['gaussian1'], label='Analysis Region')
        
        ax.set_title(f'{channel.upper()}: Peak Detection & Analysis Region', 
                     fontsize=13, fontweight='bold', color=COLORS['text'])
        ax.set_ylabel('Corrected Signal', fontsize=11)
        ax.grid(True, alpha=0.3, color=COLORS['grid'])
        ax.legend(fontsize=10, framealpha=0.9)

# === ROW 2: MODEL COMPARISON ===
for i, channel in enumerate(['xrsa', 'xrsb']):
    if channel in analysis_results and 'fitting_results' in analysis_results[channel]:
        ax = fig.add_subplot(gs[1, i*2:(i+1)*2])
        ax.set_facecolor(COLORS['background'])
        
        peak_time = analysis_results[channel]['peak_time']
        peak_data = analysis_results[channel]['peak_data']
        fitting_results = analysis_results[channel]['fitting_results']
        
        # Plot data points
        ax.plot(peak_time, peak_data, 'o', color=COLORS['data'], 
                markersize=6, alpha=0.8, label='Data Points', zorder=5)
        
        # Plot different model fits
        colors = [COLORS['gaussian1'], COLORS['gaussian2'], COLORS['gaussian3']]
        linestyles = ['-', '--', '-.']
        
        for j, (model, result) in enumerate(fitting_results.items()):
            fit_curve = result['fit']
            r2 = result['r2']
            n_comp = result['n_components']
            
            ax.plot(peak_time, fit_curve, linestyles[j], 
                    color=colors[j], linewidth=2.5, alpha=0.9,
                    label=f'{n_comp}-Gaussian (R¬≤={r2:.3f})')
        
        ax.set_title(f'{channel.upper()}: Model Comparison', 
                     fontsize=13, fontweight='bold', color=COLORS['text'])
        ax.set_ylabel('Signal Amplitude', fontsize=11)
        ax.grid(True, alpha=0.3, color=COLORS['grid'])
        ax.legend(fontsize=10, framealpha=0.9)

# === ROW 3: COMPONENT DECOMPOSITION ===
for i, channel in enumerate(['xrsa', 'xrsb']):
    if (channel in analysis_results and 'best_model' in analysis_results[channel] 
        and analysis_results[channel]['best_model'] in ['double', 'triple']):
        
        ax = fig.add_subplot(gs[2, i*2:(i+1)*2])
        ax.set_facecolor(COLORS['background'])
        
        peak_time = analysis_results[channel]['peak_time']
        peak_data = analysis_results[channel]['peak_data']
        best_model = analysis_results[channel]['best_model']
        best_params = analysis_results[channel]['fitting_results'][best_model]['params']
        best_fit = analysis_results[channel]['fitting_results'][best_model]['fit']
        n_components = analysis_results[channel]['fitting_results'][best_model]['n_components']
        
        # Plot original data
        ax.plot(peak_time, peak_data, 'o', color=COLORS['data'], 
                markersize=5, alpha=0.8, label='Data Points', zorder=6)
        
        # Plot individual components
        component_colors = [COLORS['gaussian1'], COLORS['gaussian2'], COLORS['gaussian3']]
        
        for comp_idx in range(n_components):
            # Extract component parameters
            amp = best_params[comp_idx * 3]
            center = best_params[comp_idx * 3 + 1]
            width = best_params[comp_idx * 3 + 2]
            
            # Calculate individual component
            component = gaussian(peak_time, amp, center, width)
            
            # Plot component
            color = component_colors[comp_idx % len(component_colors)]
            ax.plot(peak_time, component, ':', color=color, linewidth=2.5, alpha=0.9,
                    label=f'Component {comp_idx+1} (A={amp:.3f})')
            
            # Fill under component
            ax.fill_between(peak_time, 0, component, alpha=0.25, color=color)
        
        # Plot total fit
        ax.plot(peak_time, best_fit, '-', color=COLORS['total_fit'], 
                linewidth=3, label=f'Total Fit ({n_components} Components)', zorder=4)
        
        ax.set_title(f'{channel.upper()}: {best_model.capitalize()} Gaussian Decomposition', 
                     fontsize=13, fontweight='bold', color=COLORS['text'])
        ax.set_xlabel('Time (minutes)', fontsize=11)
        ax.set_ylabel('Signal Amplitude', fontsize=11)
        ax.grid(True, alpha=0.3, color=COLORS['grid'])
        ax.legend(fontsize=9, framealpha=0.9, bbox_to_anchor=(1.02, 1), loc='upper left')

# Footer
footer_text = ('Multi-Gaussian Component Analysis ‚Ä¢ Flare Structure Decomposition ‚Ä¢ Nanoflare Identification')
fig.text(0.5, 0.02, footer_text, ha='center', fontsize=12, 
         color='#7F8C8D', alpha=0.9, style='italic')

plt.tight_layout()
plt.show()

# === RESULTS SUMMARY ===
print(f"\nüìã ANALYSIS RESULTS SUMMARY")
print(f"{'='*70}")

for channel in analysis_results.keys():
    if 'fitting_results' in analysis_results[channel]:
        print(f"\nüåü {channel.upper()} CHANNEL RESULTS:")
        
        print(f"   Peak Detection:")
        print(f"   ‚Ä¢ Total peaks: {len(analysis_results[channel]['all_peaks'])}")
        print(f"   ‚Ä¢ Highest peak time: {analysis_results[channel]['full_time'][analysis_results[channel]['peak_position']]:.2f} min")
        print(f"   ‚Ä¢ Peak amplitude: {analysis_results[channel]['peak_amplitude']:.4f}")
        
        print(f"\n   Model Fitting Results:")
        fitting_results = analysis_results[channel]['fitting_results']
        
        for model, result in fitting_results.items():
            print(f"   ‚Ä¢ {model.capitalize()}: R¬≤={result['r2']:.4f}")
        
        best_model = analysis_results[channel]['best_model']
        best_params = fitting_results[best_model]['params']
        n_components = fitting_results[best_model]['n_components']
        
        print(f"\n   ‚úÖ BEST MODEL: {best_model.capitalize()} Gaussian ({n_components} components)")
        print(f"   ‚Ä¢ Model quality: R¬≤={fitting_results[best_model]['r2']:.4f}")
        
        if n_components > 1:
            print(f"\n   Component Parameters:")
            for comp_idx in range(n_components):
                amp = best_params[comp_idx * 3]
                center = best_params[comp_idx * 3 + 1]
                width = best_params[comp_idx * 3 + 2]
                fwhm = 2.355 * abs(width)
                
                print(f"   ‚Ä¢ Component {comp_idx+1}: A={amp:.4f}, Œº={center:.2f} min, œÉ={width:.2f}, FWHM={fwhm:.2f} min")

print(f"\nüéØ SCIENTIFIC INTERPRETATION:")
print(f"   ‚úÖ Multi-component flare structure successfully identified")
print(f"   ‚úÖ Individual Gaussian components represent distinct emission processes")
print(f"   ‚úÖ Component decomposition reveals nanoflare superposition signatures")
print(f"   ‚úÖ Results ready for advanced heating mechanism analysis")

print(f"\nüíæ All results stored in 'analysis_results' dictionary")
print(f"üöÄ Analysis complete - ready for nanoflare superposition modeling!")
print(f"="*70)

# Usage information
print(f"\nüìñ USAGE INFORMATION:")
print(f"   Basic usage: python final.py")
print(f"   Custom data file: python final.py --data data/2020_xrsa_xrsb.csv")
print(f"   Custom range: python final.py --start 1000 --end 1500")
print(f"   Show config: python final.py --show-config")
print(f"   Help: python final.py --help")
print(f"="*70)